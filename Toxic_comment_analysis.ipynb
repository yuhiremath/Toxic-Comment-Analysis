{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import string\nimport pandas as pd\nimport re\nimport nltk",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30ff23fbfda8a769f0f91212a0735522f4ebf78f"
      },
      "cell_type": "code",
      "source": "data = pd.read_csv('../input/train1/train1.csv')\ntest_data = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\ndata.fillna('unknown', inplace = True)\ntest_data.fillna('unknown', inplace = True)\npd.set_option('display.max_colwidth', 100)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5d8357eba10b75dbbd0a0201eade1f562b3f7ed6"
      },
      "cell_type": "code",
      "source": "#Remove Punctuations\ndef remove_punct(text):\n    removed_punct = ''.join([char for char in text if char not in string.punctuation])\n    return removed_punct\ndata['nopunct_text'] = data['comment_text'].apply(lambda x : remove_punct(x))\ntest_data['nopunct_text'] = test_data['comment_text'].apply(lambda x : remove_punct(x))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "053d4844a08e16954c699f950509aaa2b189c413"
      },
      "cell_type": "code",
      "source": "#Tokenization\ndef tokenize(text):\n    tokenized = re.split('\\s+', text)\n    return tokenized\ndata['tokenized_text'] = data['nopunct_text'].apply(lambda x : tokenize(x))\ntest_data['tokenized_text'] = test_data['nopunct_text'].apply(lambda x : tokenize(x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ddd8f3498223ebe21af9c9d0963a27c0ca99e30"
      },
      "cell_type": "code",
      "source": "#Remove stopwords\nfrom nltk.corpus import stopwords\nstopword = stopwords.words('english')\ndef remove_stopwords(text):\n    no_stopwords = [word for word in text if word not in stopword]\n    return no_stopwords\n# print(remove_stopwords(data['tokenized_text'][1]))\ndata['nostop_text'] = data['tokenized_text'].apply(lambda x : remove_stopwords(x))\ntest_data['nostop_text'] = test_data['tokenized_text'].apply(lambda x : remove_stopwords(x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a03f76885e96e5ff9886ffcefa28d8756dc054f6"
      },
      "cell_type": "code",
      "source": "#lemmatize the text\nwn = nltk.WordNetLemmatizer()\ndef lemmatize(text):\n    lemmatized = [wn.lemmatize(word.lower()) for word in text]\n    return lemmatized\ndata['lemmatized_text'] = data['nostop_text'].apply(lambda x : lemmatize(x))\ntest_data['lemmatized_text'] = test_data['nostop_text'].apply(lambda x : lemmatize(x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d1ab2eae04023d942d1ca5cac954478adf37facb"
      },
      "cell_type": "code",
      "source": "#Stemming the text\nps = nltk.PorterStemmer()\ndef stem(text):\n    stemmed = [ps.stem(word) for word in text]\n    return stemmed\ndata['stemmed_text'] = data['lemmatized_text'].apply(lambda x : stem(x))\ntest_data['stemmed_text'] = test_data['lemmatized_text'].apply(lambda x : stem(x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d921ea96d3b45c6ae0ce752988461c23fd04d77"
      },
      "cell_type": "code",
      "source": "def clean(text):\n    clean_text=' '.join(word for word in text)\n    return clean_text\nclean_text = data['stemmed_text'].apply(lambda x: clean(x))\ntest_clean_text = test_data['stemmed_text'].apply(lambda x: clean(x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b049ad6bafa701dc4030cc07f4eb104cf3ebd79"
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(ngram_range=(1,2)).fit(clean_text)\ncount_train = vectorizer.transform(clean_text)\ncount_test = vectorizer.transform(test_clean_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e2fbd51de5ab1b19d72221ac1dab551c1d0715f"
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer().fit(clean_text)\ntfidf_train = vectorizer.transform(clean_text)\ntfidf_test = vectorizer.transform(test_clean_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fae8fea1ef6bc3169d8941426a5de672f4c834b3"
      },
      "cell_type": "code",
      "source": "import scipy.sparse as sp\ncount_tfidf_train = sp.hstack([count_train, tfidf_train])\ncount_tfidf_test = sp.hstack([count_test, tfidf_test])\ncount_tfidf_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4637bf54eb13a0e40e83b6f1dfc4f5feebedcdb6"
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\nlabels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nsubmission_df = pd.DataFrame()\nsubmission_df['id'] = test_data['id']\nfor item in labels:\n    log = LogisticRegression().fit(tfidf_train, data[item])\n    submission_df[item] = log.predict_proba(tfidf_test)[:,1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72d0cb9d3a01791d741b9510e907ead626749665"
      },
      "cell_type": "code",
      "source": "submission_df['id'] = submission_df['id'].astype('str')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e2e354a13add86c7d4c149a808d1fba8c47dd2a"
      },
      "cell_type": "code",
      "source": "submission_df.to_csv('submission.csv', index = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66e269b31d30a038b66a2fc19c880f54a8ce5f83"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65bc5ea244be7a58ea1c33f20834c67e99520b82"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cc472346846179c7e2e44c0b938853c58e789827"
      },
      "cell_type": "code",
      "source": "# #word2vec model attempt\n# from gensim.models import word2vec\n# word_matrix = data['stemmed_text']\n# word_matrix.shape\n\n# # number of features the model encodes to \n# num_features = 300\n\n# # minimum frequency of the word in the corpus\n# min_word_count = 5 \n\n# # number of workers for parallel process the model building \n# num_workers = 4\n\n# # window size around a word\n# context = 10\n\n# print(\"Training model\")\n# model = word2vec.Word2Vec(word_matrix, workers=num_workers, size = num_features, min_count=min_word_count, window=context)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "570fec897accb248903bb2741fcfd404349d3733"
      },
      "cell_type": "code",
      "source": "# word_matrix = []\n# for sentence in data['stemmed_text']:\n#     word_matrix.append([word for word in sentence if word in model.wv.vocab.keys()])\n# word_vectors = model.wv.syn0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "240895e95c2a4aaa4ae759e508044fd8f73309b6"
      },
      "cell_type": "code",
      "source": "# import numpy as np\n# missed = 0\n# features = None\n# feature_vector = np.zeros(word_vectors.shape[1])\n# for sentence in word_matrix:\n#     count = 0\n#     for word in sentence:\n#         feature_vector += model[word]\n#         count += 1\n#     if count != 0:\n#         if features is None:\n#             features = feature_vector/count\n#         else:\n#             features = np.concatenate((features, feature_vector/count))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e44643d7226c313b58d6343aa6eb8f3ffd14b9c0"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}